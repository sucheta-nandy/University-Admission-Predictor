{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from jupyterthemes import jtplot\n",
    "#jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "admission_df = pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit  \n",
       "0         1             0.92  \n",
       "1         1             0.76  \n",
       "2         1             0.72  \n",
       "3         1             0.80  \n",
       "4         0             0.65  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>330</td>\n",
       "      <td>115</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>321</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>308</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>302</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>323</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>325</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>327</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>328</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>307</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>311</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>314</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>317</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>319</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>303</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>312</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>325</td>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>328</td>\n",
       "      <td>116</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>334</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>336</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>340</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>322</td>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>298</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>295</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>310</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>320</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>311</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>327</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>316</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>308</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>304</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>309</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>318</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>325</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>321</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>323</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>328</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>304</td>\n",
       "      <td>103</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>317</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>311</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>319</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>327</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>322</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>302</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>307</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>297</td>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>298</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>300</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>301</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "5          330          115                  5  4.5   3.0  9.34         1   \n",
       "6          321          109                  3  3.0   4.0  8.20         1   \n",
       "7          308          101                  2  3.0   4.0  7.90         0   \n",
       "8          302          102                  1  2.0   1.5  8.00         0   \n",
       "9          323          108                  3  3.5   3.0  8.60         0   \n",
       "10         325          106                  3  3.5   4.0  8.40         1   \n",
       "11         327          111                  4  4.0   4.5  9.00         1   \n",
       "12         328          112                  4  4.0   4.5  9.10         1   \n",
       "13         307          109                  3  4.0   3.0  8.00         1   \n",
       "14         311          104                  3  3.5   2.0  8.20         1   \n",
       "15         314          105                  3  3.5   2.5  8.30         0   \n",
       "16         317          107                  3  4.0   3.0  8.70         0   \n",
       "17         319          106                  3  4.0   3.0  8.00         1   \n",
       "18         318          110                  3  4.0   3.0  8.80         0   \n",
       "19         303          102                  3  3.5   3.0  8.50         0   \n",
       "20         312          107                  3  3.0   2.0  7.90         1   \n",
       "21         325          114                  4  3.0   2.0  8.40         0   \n",
       "22         328          116                  5  5.0   5.0  9.50         1   \n",
       "23         334          119                  5  5.0   4.5  9.70         1   \n",
       "24         336          119                  5  4.0   3.5  9.80         1   \n",
       "25         340          120                  5  4.5   4.5  9.60         1   \n",
       "26         322          109                  5  4.5   3.5  8.80         0   \n",
       "27         298           98                  2  1.5   2.5  7.50         1   \n",
       "28         295           93                  1  2.0   2.0  7.20         0   \n",
       "29         310           99                  2  1.5   2.0  7.30         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "470        320          110                  5  4.0   4.0  9.27         1   \n",
       "471        311          103                  3  2.0   4.0  8.09         0   \n",
       "472        327          116                  4  4.0   4.5  9.48         1   \n",
       "473        316          102                  2  4.0   3.5  8.15         0   \n",
       "474        308          105                  4  3.0   2.5  7.95         1   \n",
       "475        300          101                  3  3.5   2.5  7.88         0   \n",
       "476        304          104                  3  2.5   2.0  8.12         0   \n",
       "477        309          105                  4  3.5   2.0  8.18         0   \n",
       "478        318          103                  3  4.0   4.5  8.49         1   \n",
       "479        325          110                  4  4.5   4.0  8.96         1   \n",
       "480        321          102                  3  3.5   4.0  9.01         1   \n",
       "481        323          107                  4  3.0   2.5  8.48         1   \n",
       "482        328          113                  4  4.0   2.5  8.77         1   \n",
       "483        304          103                  5  5.0   3.0  7.92         0   \n",
       "484        317          106                  3  3.5   3.0  7.89         1   \n",
       "485        311          101                  2  2.5   3.5  8.34         1   \n",
       "486        319          102                  3  2.5   2.5  8.37         0   \n",
       "487        327          115                  4  3.5   4.0  9.14         0   \n",
       "488        322          112                  3  3.0   4.0  8.62         1   \n",
       "489        302          110                  3  4.0   4.5  8.50         0   \n",
       "490        307          105                  2  2.5   4.5  8.12         1   \n",
       "491        297           99                  4  3.0   3.5  7.81         0   \n",
       "492        298          101                  4  2.5   4.5  7.69         1   \n",
       "493        300           95                  2  3.0   1.5  8.22         1   \n",
       "494        301           99                  3  2.5   2.0  8.45         1   \n",
       "495        332          108                  5  4.5   4.0  9.02         1   \n",
       "496        337          117                  5  5.0   5.0  9.87         1   \n",
       "497        330          120                  5  4.5   5.0  9.56         1   \n",
       "498        312          103                  4  4.0   5.0  8.43         0   \n",
       "499        327          113                  4  4.5   4.5  9.04         0   \n",
       "\n",
       "     Chance of Admit  \n",
       "0               0.92  \n",
       "1               0.76  \n",
       "2               0.72  \n",
       "3               0.80  \n",
       "4               0.65  \n",
       "5               0.90  \n",
       "6               0.75  \n",
       "7               0.68  \n",
       "8               0.50  \n",
       "9               0.45  \n",
       "10              0.52  \n",
       "11              0.84  \n",
       "12              0.78  \n",
       "13              0.62  \n",
       "14              0.61  \n",
       "15              0.54  \n",
       "16              0.66  \n",
       "17              0.65  \n",
       "18              0.63  \n",
       "19              0.62  \n",
       "20              0.64  \n",
       "21              0.70  \n",
       "22              0.94  \n",
       "23              0.95  \n",
       "24              0.97  \n",
       "25              0.94  \n",
       "26              0.76  \n",
       "27              0.44  \n",
       "28              0.46  \n",
       "29              0.54  \n",
       "..               ...  \n",
       "470             0.87  \n",
       "471             0.64  \n",
       "472             0.90  \n",
       "473             0.67  \n",
       "474             0.67  \n",
       "475             0.59  \n",
       "476             0.62  \n",
       "477             0.65  \n",
       "478             0.71  \n",
       "479             0.79  \n",
       "480             0.80  \n",
       "481             0.78  \n",
       "482             0.83  \n",
       "483             0.71  \n",
       "484             0.73  \n",
       "485             0.70  \n",
       "486             0.68  \n",
       "487             0.79  \n",
       "488             0.76  \n",
       "489             0.65  \n",
       "490             0.67  \n",
       "491             0.54  \n",
       "492             0.53  \n",
       "493             0.62  \n",
       "494             0.68  \n",
       "495             0.87  \n",
       "496             0.96  \n",
       "497             0.93  \n",
       "498             0.73  \n",
       "499             0.84  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets drop the serial no.\n",
    "admission_df.drop('Serial No.', axis=1, inplace=True)\n",
    "admission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Perform Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the null values\n",
    "admission_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      "GRE Score            500 non-null int64\n",
      "TOEFL Score          500 non-null int64\n",
      "University Rating    500 non-null int64\n",
      "SOP                  500 non-null float64\n",
      "LOR                  500 non-null float64\n",
      "CGPA                 500 non-null float64\n",
      "Research             500 non-null int64\n",
      "Chance of Admit      500 non-null float64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 31.3 KB\n"
     ]
    }
   ],
   "source": [
    "#Check the dataframe information\n",
    "admission_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP       LOR   \\\n",
       "count  500.000000   500.000000         500.000000  500.000000  500.00000   \n",
       "mean   316.472000   107.192000           3.114000    3.374000    3.48400   \n",
       "std     11.295148     6.081868           1.143512    0.991004    0.92545   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.00000   \n",
       "25%    308.000000   103.000000           2.000000    2.500000    3.00000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.50000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.00000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.00000   \n",
       "\n",
       "             CGPA    Research  Chance of Admit  \n",
       "count  500.000000  500.000000        500.00000  \n",
       "mean     8.576440    0.560000          0.72174  \n",
       "std      0.604813    0.496884          0.14114  \n",
       "min      6.800000    0.000000          0.34000  \n",
       "25%      8.127500    0.000000          0.63000  \n",
       "50%      8.560000    1.000000          0.72000  \n",
       "75%      9.040000    1.000000          0.82000  \n",
       "max      9.920000    1.000000          0.97000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary of the dataframe\n",
    "admission_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University Rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304.911765</td>\n",
       "      <td>100.205882</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>2.426471</td>\n",
       "      <td>7.798529</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.562059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309.134921</td>\n",
       "      <td>103.444444</td>\n",
       "      <td>2.682540</td>\n",
       "      <td>2.956349</td>\n",
       "      <td>8.177778</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.626111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315.030864</td>\n",
       "      <td>106.314815</td>\n",
       "      <td>3.308642</td>\n",
       "      <td>3.401235</td>\n",
       "      <td>8.500123</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.702901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>323.304762</td>\n",
       "      <td>110.961905</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.947619</td>\n",
       "      <td>8.936667</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.801619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>327.890411</td>\n",
       "      <td>113.438356</td>\n",
       "      <td>4.479452</td>\n",
       "      <td>4.404110</td>\n",
       "      <td>9.278082</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.888082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    GRE Score  TOEFL Score       SOP      LOR       CGPA  \\\n",
       "University Rating                                                          \n",
       "1                  304.911765   100.205882  1.941176  2.426471  7.798529   \n",
       "2                  309.134921   103.444444  2.682540  2.956349  8.177778   \n",
       "3                  315.030864   106.314815  3.308642  3.401235  8.500123   \n",
       "4                  323.304762   110.961905  4.000000  3.947619  8.936667   \n",
       "5                  327.890411   113.438356  4.479452  4.404110  9.278082   \n",
       "\n",
       "                   Research  Chance of Admit  \n",
       "University Rating                             \n",
       "1                  0.294118         0.562059  \n",
       "2                  0.293651         0.626111  \n",
       "3                  0.537037         0.702901  \n",
       "4                  0.780952         0.801619  \n",
       "5                  0.876712         0.888082  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grouping by University ranking\n",
    "df_university = admission_df.groupby(by= 'University Rating').mean()\n",
    "df_university"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Perform Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_df.hist(bin=30, figsize=(20,20),color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(admission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix= admission_df.corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= admission_df.drop(columns =['Chance of Admit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = admission_df['Chance of Admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "5      0.90\n",
       "6      0.75\n",
       "7      0.68\n",
       "8      0.50\n",
       "9      0.45\n",
       "10     0.52\n",
       "11     0.84\n",
       "12     0.78\n",
       "13     0.62\n",
       "14     0.61\n",
       "15     0.54\n",
       "16     0.66\n",
       "17     0.65\n",
       "18     0.63\n",
       "19     0.62\n",
       "20     0.64\n",
       "21     0.70\n",
       "22     0.94\n",
       "23     0.95\n",
       "24     0.97\n",
       "25     0.94\n",
       "26     0.76\n",
       "27     0.44\n",
       "28     0.46\n",
       "29     0.54\n",
       "       ... \n",
       "470    0.87\n",
       "471    0.64\n",
       "472    0.90\n",
       "473    0.67\n",
       "474    0.67\n",
       "475    0.59\n",
       "476    0.62\n",
       "477    0.65\n",
       "478    0.71\n",
       "479    0.79\n",
       "480    0.80\n",
       "481    0.78\n",
       "482    0.83\n",
       "483    0.71\n",
       "484    0.73\n",
       "485    0.70\n",
       "486    0.68\n",
       "487    0.79\n",
       "488    0.76\n",
       "489    0.65\n",
       "490    0.67\n",
       "491    0.54\n",
       "492    0.53\n",
       "493    0.62\n",
       "494    0.68\n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit, Length: 500, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the data before training the model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler_x = StandardScaler()\n",
    "X= scaler_x.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()\n",
    "y= scaler_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train and Evaluate a Linear Regression  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression_model = LinearRegression()\n",
    "LinearRegression_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375780777541867"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_LinearRegression = LinearRegression_model.score(X_test, y_test)\n",
    "accuracy_LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train and Evaluate an artificial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model = keras.Sequential()\n",
    "ANN_model.add(Dense(50,input_dim=7))\n",
    "ANN_model.add(Activation('relu'))\n",
    "ANN_model.add(Dense(150))\n",
    "ANN_model.add(Activation('relu'))\n",
    "ANN_model.add(Dropout(0.5))\n",
    "ANN_model.add(Dense(150))\n",
    "ANN_model.add(Activation('relu'))\n",
    "ANN_model.add(Dropout(0.5))\n",
    "ANN_model.add(Dense(50))\n",
    "ANN_model.add(Activation('linear'))\n",
    "ANN_model.add(Dense(1))\n",
    "ANN_model.compile(loss ='mse', optimizer='adam')\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_hist = ANN_model.fit(X_train,y_train, epochs=100, batch_size=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ANN_model.evaluate(X_test, y_test)\n",
    "accuracy_ANN = 1 - result\n",
    "print(\"Accuracy: {}\".format(accuracy_ANN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs_hist.history['loss'])\n",
    "plt.title('Model Loss Progress During Tarining')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.legend(['Training Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train and Evaluate a Decision Tree and Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DecisionTree_model = DecisionTreeRegressor()\n",
    "DecisionTree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5200842542121715"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_DecisionTree = DecisionTree_model.score(X_test, y_test)\n",
    "accuracy_DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RandomForest_model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "RandomForest_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8036893380473464"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_RandomForest = RandomForest_model.score(X_test,y_test)\n",
    "accuracy_RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Calculate Regression model KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c5b470d898>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict = LinearRegression_model.predict(X_test)\n",
    "plt.plot(y_test,y_predict, '^', color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_orig = scaler_y.inverse_transform(y_predict)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c5b4761860>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test_orig, y_predict_orig, '^', color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = X_test.shape[1]\n",
    "n=len(X_test)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.052 \n",
      "MSE =  0.0026772734537739494 \n",
      "MAE =  0.03920646012530462 \n",
      "R2 =  0.8375780777541866 \n",
      "Adjusted R2 = 0.8206086231911911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig,y_predict_orig)),'.3f'))\n",
    "MSE = mean_squared_error(y_test_orig, y_predict_orig)\n",
    "MAE = mean_absolute_error(y_test_orig,y_predict_orig)\n",
    "r2 = r2_score(y_test_orig,y_predict_orig)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "\n",
    "print('RMSE = ',RMSE, '\\nMSE = ',MSE, '\\nMAE = ',MAE, '\\nR2 = ',r2, '\\nAdjusted R2 =',adj_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
